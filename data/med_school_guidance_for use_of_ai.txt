Title: Guidance for the use of Artificial Intelligence Tools for Academic Assignments in MD Program

This guidance serves as policy for MD program courses unless otherwise indicated by specific course faculty and noted in course syllabi. For further reading on artificial intelligence (AI) tools such as ChatGPT, Google Bard, DALL-E2, etc., at Duke, see the Learning Innovations page "AI and Teaching at Duke" at learninginnovation.duke.edu/ai-and-teaching-at-duke.

    Academic Assignments. Generative AI tools such as ChatGPT, Google Bard, and other large language models (LLMs) should not be used for generating final text for an academic assignment; some formative and prewriting use of these tools is permitted with attribution. Image generators such as DALL-E2 and Midjourney are similarly permitted for pre-assignment submission use but should not be used for generating final images for an academic assignment. Courses may implement variations on this guidance in course syllabi.

        Direct use of text and images generated by AI tools in academic assignments constitutes plagiarism. Plagiarism is prohibited by the School of Medicine Code of Professional Conduct. This includes quoting, paraphrasing, or summarizing from ChatGPT or any other AI tool without attribution.

        Direct use of text and images generated by AI in academic assignments could constitute cheating. Generative AI is not a substitute for students' own critical thinking and writing skills. Use of ChatGPT, DALL-E2 or other generative AI tool may be viewed as cheating if the assignment requires students to use their own critical thinking, to solve problems, or to practice concepts or skills.

        Use of AI tools requires attribution: Unless prohibited in a specific course, students may use ChatGPT, DALL-E2 or other AI tool with attribution for brainstorming, practice, drafting, or feedback. Students must clearly identify any writing, text, or media generated by AI. Attribution should indicate how the tool was used, including brainstorming, conception, design, feedback, or refining. For example: “This assignment used ChatGPT in brainstorming and conception of my work.”

        Students are responsible for the quality and integrity of the content of their submitted work. Generative AI tools can generate false information and violate copyright law. Students should verify any information resulting from an AI tool.

    Examinations. Students may not use AI tools such as ChatGPT when taking formal examinations unless explicitly given permission by the course instructor.

    Clinical Use

        Students may not enter any protected health information (PHI) such as demographics, medical history, or laboratory results, into ChatGPT, Google Bard, DALL-E2, or other generative AI text or image tool for any purpose.

        Students must follow current guidance and policies from the Duke University Health System governing any clinical use of ChatGPT or other generative AI tools. At present, clinical use of ChatGPT is not permitted.

    Research/Scholarly Work

        Research products, such as manuscripts, abstracts, and grant proposals, that are submitted for academic credit or as an academic requirement are subject to the same policies as academic assignments:

            direct use of text generated by AI without attribution constitutes plagiarism,

            use of AI could constitute cheating if used as a substitute for students' own critical thinking,

            use of AI tools requires attribution, and

            students are responsible for the quality and integrity of the content of their submitted work

        Research products, such as manuscripts, abstracts, and grant proposals that are submitted for publication or presentation must follow journal, publisher, or conference policies regarding the use of AI.

        Students should discuss any potential use of AI tools with their mentors.

Background

    What is Chat GPT or other large language models (LLMs)?

        LLM tools are predictive language models, not discovery tools or search engines.

        They respond to prompts by generating content based on statistical regularities, effectively paraphrasing the source material.

    Strengths of ChatGPT and other AI tools

        LLM tools can stimulate thinking, overcome writer's block, or be used in prewriting activities to generate initial content that you can analyze, adapt, and revise based on your own knowledge, reading, and synthesis.

        Students and faculty should explore the use of AI tools and reflect critically on their capabilities.

    Limitations of ChatGPT and other AI tools

        The source material from the internet used to train LLM tools has limitations: it is out of date, lacks scholarly information / paywalled content, contains biased information and misinformation, and contains inaccuracies.

        ChatGPT is known to fabricate references. (1) This is a natural result of predictive text models: it is providing a set of words in an order that makes algorithmic sense, not based on information-seeking and synthesis of authentic source material.

        LLM tools do not analyze, validate, or assess source material for accuracy.

        Generative AI image tools such as DALL-E2 and Midjourney may incorporate copyright-protected material and generate images that violate copyright law.

    Generally, scientific journals have determined that AI tools should not be used to author scientific articles, but in some cases may be used in prewriting activities if documented appropriately. (2)

        Nature states that "Authorship carries with it accountability for the work, and AI tools cannot take such responsibility." Nature allows some usage of LLM tools if documented appropriately. (3)

        JAMA similarly does not allow LLM tools to qualify for authorship. If authors use LLM tools in developing a manuscript, "authors must take full responsibility for the integrity of the content generated by these tools" and acknowledge them appropriately. (4)

        Science prohibits the use of text generated by ChatGPT or other AI tools outright and indicates that violation of this policy constitutes scientific misconduct. (5)

References:

    Alkaissi, H., & McFarlane, S. I. Artificial Hallucinations in ChatGPT: Implications in Scientific Writing. Cureus. 2023: 15(2): e35179. doi:10.7759/cureus.35179 

    International Committee of Medical Journal Editors [homepage on the Internet]. Recommendations for the Conduct, Reporting, Editing and Publication of Scholarly Work in Medical Journals Accessed 7/20/2023. Available from: ICMJE.org

    Tools such as ChatGPT threaten transparent science; here are our ground rules for their use. Nature. 2023 Jan;613(7945):612. doi: 10.1038/d41586-023-00191-1.

    Flanagin A, Bibbins-Domingo K, Berkwits M, Christiansen SL. Nonhuman "Authors" and Implications for the Integrity of Scientific Publication and Medical Knowledge. JAMA. 2023 Feb 28;329(8):637-639. doi: 10.1001/jama.2023.1344.

    Thorp HH. ChatGPT is fun, but not an author. Science. 2023 Jan 27;379(6630):313. doi: 10.1126/science.adg7879. Epub 2023 Jan 26.
    
Source: https://medicine.bulletins.duke.edu/allprograms/dr/md#policies1
Download Date: January 3, 2024